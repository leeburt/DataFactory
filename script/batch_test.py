#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡æµ‹è¯•è„šæœ¬
ç”¨äºæµ‹è¯•å¤šç§ä¸åŒçš„æç¤ºå’Œä½¿ç”¨åœºæ™¯
"""

from openai import OpenAI
import json
import time
import csv
from datetime import datetime

# APIé…ç½®
API_BASE = "https://jeniya.top/v1"
API_KEY = "sk-EWCvOoyOvpl53kI3hYgFyq9vbyVWuefsWp9ODk2cYnIleDhA"
MODEL_NAME = "o4-mini"

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = OpenAI(
    api_key=API_KEY,
    base_url=API_BASE
)

# æµ‹è¯•ç”¨ä¾‹é›†åˆ
TEST_PROMPTS = [
    {
        "category": "åŸºç¡€å¯¹è¯",
        "prompts": [
            "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
            "ä»Šå¤©å¤©æ°”çœŸä¸é”™",
            "ä½ èƒ½å¸®æˆ‘åšä»€ä¹ˆï¼Ÿ",
            "è°¢è°¢ä½ çš„å¸®åŠ©",
            "å†è§"
        ]
    },
    {
        "category": "çŸ¥è¯†é—®ç­”",
        "prompts": [
            "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
            "Pythonå’ŒJavaæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
            "è§£é‡Šä¸€ä¸‹æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ",
            "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
            "åŒºå—é“¾æŠ€æœ¯çš„åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ"
        ]
    },
    {
        "category": "åˆ›æ„å†™ä½œ",
        "prompts": [
            "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—",
            "ç¼–ä¸€ä¸ªå°æ•…äº‹ï¼Œä¸»è§’æ˜¯ä¸€åªçŒ«",
            "ä¸ºä¸€å®¶å’–å•¡åº—å†™ä¸€æ®µå¹¿å‘Šæ–‡æ¡ˆ",
            "å†™ä¸€æ®µå…³äºå‹è°Šçš„æ„Ÿæ‚Ÿ",
            "åˆ›ä½œä¸€ä¸ªç§‘å¹»å°è¯´çš„å¼€å¤´"
        ]
    },
    {
        "category": "ç¼–ç¨‹ç›¸å…³",
        "prompts": [
            "ç”¨Pythonå†™ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‡½æ•°",
            "è§£é‡Šä»€ä¹ˆæ˜¯é€’å½’",
            "å¦‚ä½•ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½ï¼Ÿ",
            "ä»€ä¹ˆæ˜¯RESTful APIï¼Ÿ",
            "Gitçš„åŸºæœ¬å‘½ä»¤æœ‰å“ªäº›ï¼Ÿ"
        ]
    },
    {
        "category": "é€»è¾‘æ¨ç†",
        "prompts": [
            "å¦‚æœæ‰€æœ‰çš„çŒ«éƒ½æ€•æ°´ï¼Œè€Œæ±¤å§†æ˜¯ä¸€åªçŒ«ï¼Œé‚£ä¹ˆæ±¤å§†æ€•æ°´å—ï¼Ÿ",
            "ä¸€ä¸ªæ•°å­—åºåˆ—ï¼š2, 4, 8, 16, ?ï¼Œä¸‹ä¸€ä¸ªæ•°å­—æ˜¯ä»€ä¹ˆï¼Ÿ",
            "æœ‰3ä¸ªå¼€å…³å’Œ3ä¸ªç¯æ³¡ï¼Œä½ åªèƒ½ä¸Šæ¥¼ä¸€æ¬¡ï¼Œå¦‚ä½•ç¡®å®šå“ªä¸ªå¼€å…³æ§åˆ¶å“ªä¸ªç¯æ³¡ï¼Ÿ",
            "å¦‚æœä»Šå¤©æ˜¯æ˜ŸæœŸä¸‰ï¼Œé‚£ä¹ˆ100å¤©åæ˜¯æ˜ŸæœŸå‡ ï¼Ÿ",
            "ä¸€ä¸ªæ± å¡˜é‡Œçš„è·èŠ±æ¯å¤©ç¿»å€ï¼Œ30å¤©åé“ºæ»¡æ•´ä¸ªæ± å¡˜ï¼Œé—®ç¬¬å‡ å¤©é“ºæ»¡ä¸€åŠï¼Ÿ"
        ]
    },
    {
        "category": "ç”Ÿæ´»å»ºè®®",
        "prompts": [
            "å¦‚ä½•ä¿æŒå¥åº·çš„ç”Ÿæ´»æ–¹å¼ï¼Ÿ",
            "æ¨èå‡ æœ¬å€¼å¾—é˜…è¯»çš„ä¹¦ç±",
            "å¦‚ä½•ç®¡ç†æ—¶é—´æ›´æœ‰æ•ˆç‡ï¼Ÿ",
            "å­¦ä¹ æ–°æŠ€èƒ½çš„æœ€ä½³æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ",
            "å¦‚ä½•å¤„ç†å·¥ä½œå‹åŠ›ï¼Ÿ"
        ]
    }
]

def test_single_prompt(prompt, category, max_tokens=150, temperature=0.7):
    """æµ‹è¯•å•ä¸ªæç¤º"""
    try:
        start_time = time.time()
        
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=max_tokens,
            temperature=temperature
        )
        
        end_time = time.time()
        response_time = end_time - start_time
        
        result = {
            "category": category,
            "prompt": prompt,
            "response": response.choices[0].message.content,
            "response_time": response_time,
            "tokens_used": response.usage.total_tokens,
            "prompt_tokens": response.usage.prompt_tokens,
            "completion_tokens": response.usage.completion_tokens,
            "success": True,
            "error": None,
            "timestamp": datetime.now().isoformat()
        }
        
        return result
        
    except Exception as e:
        result = {
            "category": category,
            "prompt": prompt,
            "response": None,
            "response_time": None,
            "tokens_used": None,
            "prompt_tokens": None,
            "completion_tokens": None,
            "success": False,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }
        
        return result

def run_batch_test():
    """è¿è¡Œæ‰¹é‡æµ‹è¯•"""
    print("="*60)
    print("  OpenAI API æ‰¹é‡æµ‹è¯•")
    print("="*60)
    
    all_results = []
    total_prompts = sum(len(category["prompts"]) for category in TEST_PROMPTS)
    current_prompt = 0
    
    for category_data in TEST_PROMPTS:
        category = category_data["category"]
        prompts = category_data["prompts"]
        
        print(f"\nğŸ“ æµ‹è¯•ç±»åˆ«: {category}")
        print("-" * 40)
        
        category_results = []
        
        for prompt in prompts:
            current_prompt += 1
            print(f"[{current_prompt}/{total_prompts}] æµ‹è¯•: {prompt[:50]}{'...' if len(prompt) > 50 else ''}")
            
            result = test_single_prompt(prompt, category)
            all_results.append(result)
            category_results.append(result)
            
            if result["success"]:
                print(f"  âœ… æˆåŠŸ | ç”¨æ—¶: {result['response_time']:.2f}s | Token: {result['tokens_used']}")
                print(f"  å›å¤: {result['response'][:100]}{'...' if len(result['response']) > 100 else ''}")
            else:
                print(f"  âŒ å¤±è´¥: {result['error']}")
            
            print()
            time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        # ç±»åˆ«ç»Ÿè®¡
        successful = sum(1 for r in category_results if r["success"])
        print(f"ğŸ“Š {category} ç±»åˆ«ç»Ÿè®¡: {successful}/{len(prompts)} æˆåŠŸ")
        
        if successful > 0:
            avg_time = sum(r["response_time"] for r in category_results if r["success"]) / successful
            avg_tokens = sum(r["tokens_used"] for r in category_results if r["success"]) / successful
            print(f"   å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}ç§’")
            print(f"   å¹³å‡Tokenä½¿ç”¨: {avg_tokens:.1f}")
    
    return all_results

def generate_report(results):
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    print("\n" + "="*60)
    print("  æµ‹è¯•æŠ¥å‘Š")
    print("="*60)
    
    # æ€»ä½“ç»Ÿè®¡
    total_tests = len(results)
    successful_tests = sum(1 for r in results if r["success"])
    failed_tests = total_tests - successful_tests
    
    print(f"\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
    print(f"  æ€»æµ‹è¯•æ•°: {total_tests}")
    print(f"  æˆåŠŸ: {successful_tests}")
    print(f"  å¤±è´¥: {failed_tests}")
    print(f"  æˆåŠŸç‡: {(successful_tests/total_tests)*100:.1f}%")
    
    if successful_tests > 0:
        # æ€§èƒ½ç»Ÿè®¡
        successful_results = [r for r in results if r["success"]]
        avg_response_time = sum(r["response_time"] for r in successful_results) / len(successful_results)
        avg_tokens = sum(r["tokens_used"] for r in successful_results) / len(successful_results)
        total_tokens = sum(r["tokens_used"] for r in successful_results)
        
        print(f"\nâš¡ æ€§èƒ½ç»Ÿè®¡:")
        print(f"  å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}ç§’")
        print(f"  å¹³å‡Tokenä½¿ç”¨: {avg_tokens:.1f}")
        print(f"  æ€»Tokenæ¶ˆè€—: {total_tokens}")
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        print(f"\nğŸ“Š åˆ†ç±»åˆ«ç»Ÿè®¡:")
        categories = {}
        for result in results:
            category = result["category"]
            if category not in categories:
                categories[category] = {"total": 0, "success": 0}
            categories[category]["total"] += 1
            if result["success"]:
                categories[category]["success"] += 1
        
        for category, stats in categories.items():
            success_rate = (stats["success"] / stats["total"]) * 100
            print(f"  {category}: {stats['success']}/{stats['total']} ({success_rate:.1f}%)")
    
    # å¤±è´¥åˆ†æ
    if failed_tests > 0:
        print(f"\nâŒ å¤±è´¥åˆ†æ:")
        failed_results = [r for r in results if not r["success"]]
        error_types = {}
        for result in failed_results:
            error = result["error"]
            if error not in error_types:
                error_types[error] = 0
            error_types[error] += 1
        
        for error, count in error_types.items():
            print(f"  {error}: {count}æ¬¡")

def save_results_to_csv(results, filename=None):
    """ä¿å­˜ç»“æœåˆ°CSVæ–‡ä»¶"""
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"batch_test_results_{timestamp}.csv"
    
    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = [
            'timestamp', 'category', 'prompt', 'response', 
            'success', 'error', 'response_time', 'tokens_used',
            'prompt_tokens', 'completion_tokens'
        ]
        
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        
        for result in results:
            writer.writerow(result)
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}")

def save_results_to_json(results, filename=None):
    """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"batch_test_results_{timestamp}.json"
    
    with open(filename, 'w', encoding='utf-8') as jsonfile:
        json.dump(results, jsonfile, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}")

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹æ‰¹é‡æµ‹è¯•...")
    
    # è¿è¡Œæµ‹è¯•
    results = run_batch_test()
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_report(results)
    
    # ä¿å­˜ç»“æœ
    save_results_to_csv(results)
    save_results_to_json(results)
    
    print(f"\nğŸ‰ æ‰¹é‡æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    main() 